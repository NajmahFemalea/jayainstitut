# -*- coding: utf-8 -*-
"""submissionAkhir_pendidikanJayaJaya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OhfNLr7KO8IL8DUOhK5CuNL6-vssvERF

# Proyek Akhir: Menyelesaikan Permasalahan Institusi Pendidikan Jaya Jaya Institut

- Nama: Najmah Femalea
- Email: najmahfemalea15@gmail.com
- Id Dicoding: najmah_femalea

## Persiapan

### Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import resample, shuffle
from sklearn.pipeline import Pipeline
import joblib

"""### Menyiapkan data yang akan diguankan

## Data Understanding
"""

# import dataset
df = pd.read_csv("data.csv", sep=";")
df.head()

# mengecek missing value
df.isna().sum()

"""Dari output diatas, tidak terdapat missing value pada data."""

# melihat type data
df.info()

"""Jika dilihat dari data diatas:
-  nilai dari kolom **Curricular_units_1st_sem_grade** dan kolom **Curricular_units_2nd_sem_grade** memiliki type data float yang dimana kita akan membulatkannya menjadi 2 angka dibelakang koma saja.
- Terdapat ambigu/typo pada nama kolom **nacionality** yang seharusnya nationality, kita harus ganti nama kolom tersebut
"""

# membulatkan nilai dari kolom curricular_units_1st_sem_grade dan curricular_units_2nd_sem_grade
rounded_columns = ['Curricular_units_1st_sem_grade',
                   'Curricular_units_2nd_sem_grade']
df.loc[:, rounded_columns] = df[['Curricular_units_1st_sem_grade',
                                 'Curricular_units_2nd_sem_grade']].round(2)

# ganti nama kolom dari nacionality
df.rename(columns={"Nacionality": "Nationality"}, inplace=True)

# lihat kembali datanya
df.head()

"""Karena semua datanya sudah berbentuk numerik, kita ubah terlebih dahulu ke tipe data **category**. Dengan dtype='category' kita memberi tahu bahwa angka-angka itu bukan nilai kuantitatif yang “bisa dijumlah” atau “dihitung rata-rata,” melainkan label diskrit."""

# sesuaikan tipe data
categorical_column = ["Marital_status", "Application_mode", "Course", "Daytime_evening_attendance", "Previous_qualification", "Nationality", "Mothers_qualification", "Fathers_qualification",
                      "Mothers_occupation", "Fathers_occupation", "Displaced", "Educational_special_needs", "Debtor", "Tuition_fees_up_to_date", "Gender", "Scholarship_holder", "International", "Status"]

ordinal_column = ["Application_order"]

numeric_column = ["Previous_qualification_grade", "Admission_grade", "Age_at_enrollment", "Curricular_units_1st_sem_credited", "Curricular_units_1st_sem_enrolled", "Curricular_units_1st_sem_evaluations", "Curricular_units_1st_sem_approved", "Curricular_units_1st_sem_grade",
                  "Curricular_units_1st_sem_without_evaluations", "Curricular_units_2nd_sem_credited", "Curricular_units_2nd_sem_enrolled", "Curricular_units_2nd_sem_evaluations", "Curricular_units_2nd_sem_approved", "Curricular_units_2nd_sem_grade", "Curricular_units_2nd_sem_without_evaluations", "Unemployment_rate", "Inflation_rate",	"GDP"]

# mengubah tipe data menjadi category
df[categorical_column + ordinal_column] = df[categorical_column + ordinal_column].astype('category')

# melihat apakah sudah terganti
df.info()

# melihat rangkuman parameter statistik
df.describe(include="all")

"""## Data Preparation / Preprocessing"""

# melihat distribusi dari data attrition
print(df['Status'].value_counts())

plt.figure(figsize=(6, 4))
sns.countplot(x='Status', data=df, palette='pastel')
plt.title('Status Class Distribution')
plt.show()

# melihat distribusi feature category

for col in categorical_column:
  plt.figure(figsize=(12,4))
  sns.countplot(data=df, x=col)
  plt.title(f"Distribution of {col}")
  plt.show()

# melihat distribusi feature numeric
for col in numeric_column:
    plt.figure(figsize=(10, 4))
    sns.histplot(df[col], bins=30, kde=False)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()

# melihat korelasi dari feature numeric
plt.figure(figsize=(15, 15))
sns.heatmap(df[numeric_column + ordinal_column].corr(), annot=True, cmap='coolwarm', linecolor="black", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""### Feature Selection

Berdasarkan heatmap diatas, terdapat beberapa kolom yang memiliki multikolinearitas. Untuk menghilangkan redundancy, mengurangi multikolinearitas, dan membuat model lebih stabil serta interpretasi koefisien menjadi bermakna. Dan memilih kolom yang tidak berkorelasi tinggi.
"""

final_features = [
    "Previous_qualification_grade",
    "Admission_grade",
    "Age_at_enrollment",
    "Curricular_units_1st_sem_approved",
    "Application_order",
    "Application_mode",
    "Debtor",
    "Scholarship_holder",
    "Tuition_fees_up_to_date",
    "Daytime_evening_attendance",
    "Gender",
    "Displaced"
]

"""### Train Test - Split"""

# Cek jumlah data
print(f"Total rows in df: {len(df)}")

# memilih X dan y
target = "Status"
df = df[final_features + [target]].copy()

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)
print(train_df.shape)
print(test_df.shape)

"""### Oversampling"""

# melihat distribusi dari target "Status"
sns.countplot(data=train_df, x="Status", hue="Status")
plt.show()

"""Karena imbalance data, kita akan melakukan teknik oversampling untuk menangani imbalanced data yang terdapat dalam data latih."""

train_df['Status'].value_counts()

# mengelompokkan data yang mengandung majoritas dan minoritas
df_minority_1 = train_df[(train_df.Status == "Dropout")]
df_minority_2 = train_df[(train_df.Status == "Enrolled")]
df_majority = train_df[(train_df.Status == "Graduate")]

# proses sampling
df_minority_1_oversampled = resample(df_minority_1, replace=True, n_samples=1791, random_state=42)
df_minority_2_oversampled = resample(df_minority_2, replace=True, n_samples=1791, random_state=42)
print(df_minority_1_oversampled.shape)
print(df_minority_2_oversampled.shape)

# menggabungkan kembali seluruh data tersebut dan diacak
oversampled_train_df = pd.concat([df_majority, df_minority_1_oversampled]).reset_index(drop=True)
oversampled_train_df = pd.concat([oversampled_train_df, df_minority_2_oversampled]).reset_index(drop=True)
oversampled_train_df = shuffle(oversampled_train_df, random_state=42)
oversampled_train_df.reset_index(drop=True, inplace=True)
oversampled_train_df.sample(5)

# cek balance dari data
sns.countplot(data=oversampled_train_df, x="Status")
plt.show()

"""### Split X dan y"""

X_train = oversampled_train_df[final_features]
y_train = oversampled_train_df["Status"]

X_test = test_df[final_features]
y_test = test_df["Status"]

"""### Encoding dan Scaling"""

ordinal_encoder = OrdinalEncoder()
onehot_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
scaler = StandardScaler()

featuretransformer = ColumnTransformer(transformers=[
    ("num", scaler, [item for item in numeric_column if item in final_features]),
    ("cat", onehot_encoder, [item for item in categorical_column if item in final_features]),
    ("ord", ordinal_encoder, [item for item in ordinal_column if item in final_features])
])

X_train_processed = featuretransformer.fit_transform(X_train)
X_test_processed = featuretransformer.transform(X_test)

joblib.dump(featuretransformer, "/content/featuretransformer.joblib")

feature_columns = X_train.columns.tolist()
joblib.dump(feature_columns, "/content/feature_columns.joblib")

def get_features_name(ct):
    output_features = []

    for name, transformer, columns in ct.transformers_:
        if name == "remainder":
            continue
        if hasattr(transformer, "get_feature_names_out"):
            feature_names = transformer.get_feature_names_out(columns)
        else:
            feature_names = columns
        output_features.extend(feature_names)
    return output_features

features_name = get_features_name(featuretransformer)

l_encoder = LabelEncoder()
y_train_encoded = l_encoder.fit_transform(y_train)
y_test_encoded = l_encoder.transform(y_test)

joblib.dump(l_encoder, "/content/label_encoder.joblib")

"""## Modeling

beberapa algoritma yang akan kita gunakan.

- Decision tree
- Random forest
- Gradient Boosting

Selain beberapa algoritma machine learning di atas, kita juga akan menggunakan algoritma grid search untuk melakukan mencari parameter paling optimal dari suatu model.

## Decision Tree
"""

tree_model = DecisionTreeClassifier(random_state=123)

param_grid = {
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_tree = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_tree.fit(X_train_processed, y_train_encoded)

print("best parameters: ", CV_tree.best_params_)

# gunakan hyperparameter yg diatas untuk membuat model
tree_model = DecisionTreeClassifier(
    random_state=123,
    criterion='gini',
    max_depth=8,
    max_features='sqrt'
)

tree_model.fit(X_train_processed, y_train_encoded)
joblib.dump(tree_model, "/content/tree_model.joblib")

"""## Random Forest"""

rdf_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_rdf = GridSearchCV(estimator=rdf_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_rdf.fit(X_train_processed, y_train_encoded)

print("best parameters: ", CV_rdf.best_params_)

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=200,
    max_features='sqrt',
    criterion='gini',
    n_jobs=-1
)
rdf_model.fit(X_train_processed, y_train_encoded)
joblib.dump(rdf_model, "/content/rdf_model.joblib")

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gboost_model = GradientBoostingClassifier(random_state=123)

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}

CV_gboost = GridSearchCV(estimator=gboost_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_gboost.fit(X_train_processed, y_train_encoded)

print("best parameters: ", CV_gboost.best_params_)

gboost_model = GradientBoostingClassifier(
    random_state=123,
    learning_rate=0.1,
    max_depth=8,
    max_features='sqrt',
    n_estimators=200
)
gboost_model.fit(X_train_processed, y_train_encoded)
joblib.dump(gboost_model, "/content/gboost_model.joblib")

"""## Evaluation"""

def evaluating(y_pred, y_true):
    '''Evaluasi model'''
    # Ambil kembali label string dari encoder
    labels = l_encoder.classes_

    # Classification report
    print(classification_report(y_true, y_pred, target_names=labels))

    # Confusion matrix
    cnf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)
    confusion_matrix_df = pd.DataFrame(cnf_matrix, index=labels, columns=labels)

    sns.heatmap(confusion_matrix_df, annot=True, fmt='d', cmap='YlGnBu', annot_kws={'size': 14})
    plt.ylabel('True Labels', fontsize=15)
    plt.xlabel('Predicted Labels', fontsize=15)
    plt.show()

    return confusion_matrix_df

"""## Gradient Boosting"""

# Prediksi menggunakan model
y_pred_test = gboost_model.predict(X_test_processed)

evaluating(y_pred=y_pred_test, y_true=y_test_encoded)

"""## Decision Tree"""

y_pred_test = tree_model.predict(X_test_processed)
evaluating(y_pred=y_pred_test, y_true=y_test_encoded)

"""## Random Forest"""

y_pred_test = rdf_model.predict(X_test_processed)
evaluating(y_pred=y_pred_test, y_true=y_test_encoded)